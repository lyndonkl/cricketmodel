{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cricket GNN Hyperparameter Search on Colab\n\nThis notebook runs hyperparameter optimization for the Cricket Ball Prediction GNN model using a free T4 GPU on Google Colab.\n\n**Prerequisites:**\n- Google account\n- WandB account (free at wandb.ai)\n- GitHub Personal Access Token (for private repo access) - [Create one here](https://github.com/settings/tokens)\n\n**Important:** Colab sessions have a 12-hour limit. Save your results frequently!\n\nFor detailed instructions, see `notes/training-guides/colab-guide.md`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Check & Setup\n",
    "\n",
    "First, verify that a GPU is available. You should see a T4 GPU listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\\n** WARNING: No GPU detected! **\")\n",
    "    print(\"Go to Runtime > Change runtime type > Hardware accelerator > T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Clone Repository\n\nClone the Cricket GNN repository from GitHub. Since this is a private repo, you'll need a GitHub Personal Access Token (PAT).\n\n**To create a PAT:**\n1. Go to [GitHub Settings > Tokens](https://github.com/settings/tokens)\n2. Click \"Generate new token (classic)\"\n3. Give it a name, select `repo` scope, and generate\n4. Copy the token (you won't see it again)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from getpass import getpass\nimport os\n\n# Securely input your GitHub PAT (won't be displayed)\ngithub_token = getpass(\"Enter your GitHub Personal Access Token: \")\n\n# Clone using the token\n!git clone https://{github_token}@github.com/lyndonkl/cricketmodel.git\n\n# Clear token from memory\ndel github_token\n\n%cd cricketmodel\n!git log --oneline -3  # Show recent commits"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies\n",
    "\n",
    "Install PyTorch Geometric and other required packages. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install PyTorch with CUDA (matching local environment versions)\n!pip install torch==2.8.0 --index-url https://download.pytorch.org/whl/cu121\n\n# Install torch-geometric (matching local version)\n!pip install torch-geometric==2.7.0\n\n# Install Optuna and WandB for hyperparameter search\n!pip install optuna optuna-integration[wandb] wandb\n\n# Install other dependencies\n!pip install tqdm pyyaml scikit-learn plotly\n\n# Verify versions\nimport torch\nimport torch_geometric\nprint(\"\\n\" + \"=\"*50)\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"torch-geometric: {torch_geometric.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(\"=\"*50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WandB Login\n",
    "\n",
    "Login to Weights & Biases for experiment tracking. You'll need your API key from [wandb.ai/authorize](https://wandb.ai/authorize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# This will prompt you to enter your API key\n",
    "# Get it from: https://wandb.ai/authorize\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Load Processed Data from Google Drive\n\nThe processed data (~97 GB) should be uploaded to your Google Drive as `processed.zip`.\n\n**One-time setup (on your local machine):**\n```bash\ncd data && zip -r processed.zip processed/\n# Upload processed.zip to Google Drive\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\nimport os\n\n# Mount Google Drive\ndrive.mount('/content/drive')\n\n# Path to your processed.zip in Google Drive (adjust if in a subfolder)\ndrive_zip_path = '/content/drive/MyDrive/processed.zip'\n\n# Check if file exists\nif os.path.exists(drive_zip_path):\n    print(f\"Found {drive_zip_path}\")\n    print(\"Extracting to data/processed/ ...\")\n    !unzip -q \"{drive_zip_path}\" -d data/\n    \n    # Verify extraction\n    file_count = len([f for f in os.listdir('data/processed') if f.endswith('.pt')])\n    print(f\"Extracted {file_count} .pt files to data/processed/\")\nelse:\n    print(f\"ERROR: {drive_zip_path} not found!\")\n    print(\"Please upload processed.zip to your Google Drive root folder.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Hyperparameter Search (4 Phases)\n\nThe HP search runs in 4 phases, each building on the previous results:\n\n| Phase | Parameters | Purpose | Trials |\n|-------|------------|---------|--------|\n| 1. Coarse | hidden_dim, lr | Find ballpark model size and learning rate | 10 |\n| 2. Architecture | num_layers, num_heads | Tune depth and attention | 12 |\n| 3. Training | lr, dropout, weight_decay | Regularization tuning | 15 |\n| 4. Loss | focal_gamma, use_class_weights | Loss function tuning | 10 |\n\n**Why `--n-jobs 1`?** On GPU, parallel jobs compete for VRAM. The speedup comes from the GPU itself, not parallelism. Use `--n-jobs > 1` only for CPU training.\n\n**Time estimate:** ~1-2 hours per phase with default settings."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Phase 1: Coarse search - find ballpark hidden_dim and learning rate\n# Adjust --n-trials and --epochs based on your time budget\n\n!python scripts/hp_search.py \\\n    --phase phase1_coarse \\\n    --n-trials 10 \\\n    --epochs 25 \\\n    --wandb \\\n    --device cuda \\\n    --n-jobs 1\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Phase 1 complete! Run the next cell to see results.\")\nprint(\"=\"*50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Phase 1 Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport glob\nimport os\n\n# Find the most recent Phase 1 best_params.json\nphase1_files = glob.glob('checkpoints/optuna/cricket_gnn_phase1_coarse_*/best_params.json')\n\nif phase1_files:\n    PHASE1_BEST = max(phase1_files, key=os.path.getmtime)\n    print(f\"Phase 1 results: {PHASE1_BEST}\")\n    print(\"=\"*50)\n    \n    with open(PHASE1_BEST) as f:\n        best_params = json.load(f)\n    \n    for key, value in best_params.items():\n        print(f\"{key}: {value}\")\nelse:\n    print(\"No Phase 1 results found. Run Phase 1 first.\")\n    PHASE1_BEST = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Phase 2: Architecture Tuning\n\nTune num_layers and num_heads using Phase 1's best hidden_dim and lr."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Phase 2: Architecture tuning - num_layers and num_heads\nif PHASE1_BEST:\n    !python scripts/hp_search.py \\\n        --phase phase2_architecture \\\n        --n-trials 12 \\\n        --epochs 25 \\\n        --best-params \"{PHASE1_BEST}\" \\\n        --wandb \\\n        --device cuda \\\n        --n-jobs 1\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Phase 2 complete!\")\n    print(\"=\"*50)\nelse:\n    print(\"ERROR: Run Phase 1 first!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Phase 3: Training Dynamics\n\nFine-tune learning rate, dropout, and weight_decay."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find Phase 2 results\nphase2_files = glob.glob('checkpoints/optuna/cricket_gnn_phase2_architecture_*/best_params.json')\n\nif phase2_files:\n    PHASE2_BEST = max(phase2_files, key=os.path.getmtime)\n    print(f\"Using Phase 2 results: {PHASE2_BEST}\")\n    \n    # Phase 3: Training dynamics - lr, dropout, weight_decay\n    !python scripts/hp_search.py \\\n        --phase phase3_training \\\n        --n-trials 15 \\\n        --epochs 30 \\\n        --best-params \"{PHASE2_BEST}\" \\\n        --wandb \\\n        --device cuda \\\n        --n-jobs 1\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Phase 3 complete!\")\n    print(\"=\"*50)\nelse:\n    print(\"ERROR: Run Phase 2 first!\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 4: Loss Function Tuning\n\nOptimize focal_gamma and class weighting.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Find Phase 3 results\nphase3_files = glob.glob('checkpoints/optuna/cricket_gnn_phase3_training_*/best_params.json')\n\nif phase3_files:\n    PHASE3_BEST = max(phase3_files, key=os.path.getmtime)\n    print(f\"Using Phase 3 results: {PHASE3_BEST}\")\n    \n    # Phase 4: Loss function - focal_gamma, use_class_weights\n    !python scripts/hp_search.py \\\n        --phase phase4_loss \\\n        --n-trials 10 \\\n        --epochs 30 \\\n        --best-params \"{PHASE3_BEST}\" \\\n        --wandb \\\n        --device cuda \\\n        --n-jobs 1\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Phase 4 complete! All phases done.\")\n    print(\"=\"*50)\nelse:\n    print(\"ERROR: Run Phase 3 first!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Final Results\n\nDisplay the best hyperparameters from all phases.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Show final results from Phase 4 (or latest completed phase)\nfor phase_name, pattern in [\n    (\"Phase 4 (Loss)\", \"phase4_loss\"),\n    (\"Phase 3 (Training)\", \"phase3_training\"),\n    (\"Phase 2 (Architecture)\", \"phase2_architecture\"),\n    (\"Phase 1 (Coarse)\", \"phase1_coarse\"),\n]:\n    files = glob.glob(f'checkpoints/optuna/cricket_gnn_{pattern}_*/best_params.json')\n    if files:\n        latest = max(files, key=os.path.getmtime)\n        print(f\"=== {phase_name} ===\")\n        print(f\"File: {latest}\")\n        with open(latest) as f:\n            params = json.load(f)\n        for k, v in params.items():\n            print(f\"  {k}: {v}\")\n        print()\n        break\nelse:\n    print(\"No results found. Run the HP search phases first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Download Results\n\nDownload all checkpoints and results to your local machine.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from google.colab import files\n\n# Create results archive\n!zip -r results.zip checkpoints/optuna/ optuna_studies.db\n\n# Show archive contents\n!unzip -l results.zip | head -30\nprint(\"...\")\n\n# Get file size\nsize_mb = os.path.getsize('results.zip') / 1e6\nprint(f\"\\nTotal archive size: {size_mb:.1f} MB\")\n\n# Download\nprint(\"\\nStarting download...\")\nfiles.download('results.zip')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Troubleshooting\n\n**Git clone fails (private repo):**\n- Error: `could not read Username for 'https://github.com'`\n- Solution: Use a GitHub Personal Access Token (PAT) - the notebook prompts for this securely\n- Create a PAT at [github.com/settings/tokens](https://github.com/settings/tokens) with `repo` scope\n\n**GPU not detected:**\n- Go to Runtime > Change runtime type > Hardware accelerator > T4 GPU\n\n**Out of memory:**\n- Reduce batch size: add `--batch-size 32` to the hp_search command\n- Restart runtime: Runtime > Restart runtime\n\n**Session disconnected:**\n\nColab has two timeout mechanisms:\n- **Browser inactivity (~90 min):** If your browser tab is idle (no interaction), Colab disconnects\n- **Max runtime (12 hours):** Hard limit regardless of activity\n\nTo prevent idle disconnects: keep the tab visible, occasionally scroll/click, don't let your computer sleep.\n\nTo recover:\n1. Click \"Reconnect\" in the toolbar\n2. Re-run cells 1-5 (setup)\n3. Cell 6 will use cached data if available\n4. Your Optuna study is saved to SQLite and resumes automatically\n\n**WandB errors:**\n- Run `wandb.login()` again if your session expired\n\n**Why pip instead of conda?**\n\nColab uses pip, not conda. Many packages (torch, numpy, pandas, sklearn, matplotlib) are pre-installed. We only install what's missing: torch-geometric, optuna, wandb, plotly."
  }
 ]
}